# Pipeline Module

The `pipeline` module is responsible for performing ETL processing on the data inputted from the LMNH Botany API. The data is requested from the API, cleaned and transformed to fit the desired schema, and loaded into the database.

- `pipeline.py`
    1. Requests data asynchronously from the LMNH Botany API
    2. Validate and select data that fits the set of expected values for a plant's status
    3. Load the cleaned data into the database (executemany)

- `logger_config.py`
    - Contains the configuration parameters for logging messages to the shell for testing/diagnostic purposes

- `Dockerfile`
  1. Copies the necessary python libraries to a python environment
  2. Creates a docker image using the script `pipeline.py` 


- `pipeline_ecr_push.sh`
**If the intended ecr `c15-cacareco-lmnh-plants-etl` is not used, replace the commands with those generated by the new ECR**
1. Signs into aws using the `aws` CLI
2. Builds, tags and pushes the new docker image to the remote ECR.

## Setup

1. Install Docker on your system. Ensure the docker daemon is running when attempting to dockerize.
2. Install and initialize terraform
3. Build terraform resources
```
terraform init
terraform plan # Preview changes
terraform apply
```

## Virtual Environment Installation
Install required libraries into a `venv`
```
pip install -r requirements.txt
```
Requirements:
- requests
- pyodbc
- pymssql
- python-dotenv
- boto3
- aiohttp
- altair
- pandas

## Deployment
Requirements for deploying 
1. Running the terraform commands to create the required resources 
2. Create a remote ECR
3. Push the docker image to the remote ECR.
3. (optional) Test the lambda function on AWS
